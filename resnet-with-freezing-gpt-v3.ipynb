{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8232682,"sourceType":"datasetVersion","datasetId":4882457}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers torchvision datasets --quiet\nimport torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom torch.optim import AdamW\nfrom torchvision.models import resnet18\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport os\nfrom torchvision import transforms\nfrom torchmetrics.text import CharErrorRate, WordErrorRate\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T10:23:08.559803Z","iopub.execute_input":"2025-05-12T10:23:08.560336Z","iopub.status.idle":"2025-05-12T10:24:40.964631Z","shell.execute_reply.started":"2025-05-12T10:23:08.560313Z","shell.execute_reply":"2025-05-12T10:24:40.963758Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-05-12 10:24:28.726767: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747045468.912809      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747045468.965934      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchvision import transforms\nfrom torchvision.models import resnet18\nfrom torchmetrics.text import CharErrorRate, WordErrorRate\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\n# Dataset Class\nclass HandwritingDataset(Dataset):\n    def __init__(self, images_dir, labels_dir, tokenizer, transform=None):\n        self.images_dir = images_dir\n        self.labels_dir = labels_dir\n        self.tokenizer = tokenizer\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((128, 128)),\n            transforms.ToTensor(),\n        ])\n\n        self.image_files = sorted([f for f in os.listdir(images_dir) \n                                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n        self.label_files = sorted([f for f in os.listdir(labels_dir) \n                                   if f.lower().endswith('.txt')])\n\n        assert len(self.image_files) == len(self.label_files), \"Image/label count mismatch\"\n        for img, lbl in zip(self.image_files, self.label_files):\n            assert os.path.splitext(img)[0] == os.path.splitext(lbl)[0], \\\n                f\"Mismatched pair: {img} vs {lbl}\"\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.images_dir, self.image_files[idx])\n        image = Image.open(img_path).convert('RGB')\n\n        lbl_path = os.path.join(self.labels_dir, self.label_files[idx])\n        with open(lbl_path, 'r', encoding='windows-1256') as f:\n            text = f.read().strip()\n\n        if self.transform:\n            image = self.transform(image)\n\n        inputs = self.tokenizer(\n            text,\n            return_tensors='pt',\n            padding='max_length',\n            max_length=128,\n            truncation=True\n        )\n\n        return {\n            'pixel_values': image,\n            'input_ids': inputs['input_ids'].squeeze(0),\n            'attention_mask': inputs['attention_mask'].squeeze(0),\n            'raw_text': text\n        }\n\n# Model with Frozen ResNet\nclass HandwritingGPT2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cnn = resnet18(pretrained=True)\n        for param in self.cnn.parameters():\n            param.requires_grad = False\n        self.cnn.fc = nn.Linear(512, 768)\n        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"aubmindlab/aragpt2-base\")\n\n    def forward(self, pixel_values, input_ids=None, attention_mask=None, labels=None):\n        features = self.cnn(pixel_values)\n        if input_ids is not None:\n            features = features.unsqueeze(1).expand(-1, input_ids.shape[1], -1)\n        return self.gpt2(\n            inputs_embeds=features,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n# Initialize\ntokenizer = GPT2Tokenizer.from_pretrained(\"aubmindlab/aragpt2-base\")\ntokenizer.pad_token = tokenizer.eos_token\n\ndataset = HandwritingDataset(\n    images_dir=\"/kaggle/input/khatt-arabic-hand-written-lines/images\",\n    labels_dir=\"/kaggle/input/khatt-arabic-hand-written-lines/labels\",\n    tokenizer=tokenizer\n)\n\ntrain_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)\ntrain_dataset = Subset(dataset, train_idx)\nval_dataset = Subset(dataset, val_idx)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = HandwritingGPT2().to(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)\ncer = CharErrorRate().to(device)\nwer = WordErrorRate().to(device)\n\n# Training Loop\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    train_cer, train_wer = [], []\n\n    for batch in train_loader:\n        pixel_values = batch['pixel_values'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(pixel_values, input_ids, attention_mask, labels=input_ids)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n        with torch.no_grad():\n            features = model.cnn(pixel_values).unsqueeze(1)\n            generated = model.gpt2.generate(\n                inputs_embeds=features,\n                max_length=128,\n                num_beams=5,\n                early_stopping=True,\n                pad_token_id=tokenizer.pad_token_id\n            )\n            preds = [tokenizer.decode(g, skip_special_tokens=True) for g in generated]\n            train_cer.append(cer(preds, batch['raw_text']))\n            train_wer.append(wer(preds, batch['raw_text']))\n\n    print(f\"\\n--- Epoch {epoch+1} Training ---\")\n    print(f\"Avg Loss     : {total_loss / len(train_loader):.4f}\")\n    print(f\"Training CER : {torch.stack(train_cer).mean().item():.4f}\")\n    print(f\"Training WER : {torch.stack(train_wer).mean().item():.4f}\")\n\n    model.eval()\n    val_loss, val_cer, val_wer = 0, [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            pixel_values = batch['pixel_values'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n\n            outputs = model(pixel_values, input_ids, attention_mask, labels=input_ids)\n            val_loss += outputs.loss.item()\n            features = model.cnn(pixel_values).unsqueeze(1)\n            generated = model.gpt2.generate(\n                inputs_embeds=features,\n                max_length=128,\n                num_beams=5,\n                early_stopping=True,\n                pad_token_id=tokenizer.pad_token_id\n            )\n            preds = [tokenizer.decode(g, skip_special_tokens=True) for g in generated]\n            val_cer.append(cer(preds, batch['raw_text']))\n            val_wer.append(wer(preds, batch['raw_text']))\n\n    print(f\"--- Epoch {epoch+1} Validation ---\")\n    print(f\"Validation Loss : {val_loss / len(val_loader):.4f}\")\n    print(f\"Validation CER  : {torch.stack(val_cer).mean().item():.4f}\")\n    print(f\"Validation WER  : {torch.stack(val_wer).mean().item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T10:24:40.965861Z","iopub.execute_input":"2025-05-12T10:24:40.966409Z","iopub.status.idle":"2025-05-12T14:14:14.555148Z","shell.execute_reply.started":"2025-05-12T10:24:40.966387Z","shell.execute_reply":"2025-05-12T14:14:14.554353Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ebe86d1e5794c64bbed8573c0a5d899"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"838307183d4649a2a3d195a21bd45cae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/4.52M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4c102b46c4145eb9971038087387c36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d9f3be126b74e5cbf9af3c177cbcd84"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 190MB/s]\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6542d6c7a54f42e393dc38b1a689cc4d"}},"metadata":{}},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 1 Training ---\nAvg Loss     : 1.2727\nTraining CER : 0.9510\nTraining WER : 0.9902\n--- Epoch 1 Validation ---\nValidation Loss : 4.7213\nValidation CER  : 1.9752\nValidation WER  : 1.4356\n\n--- Epoch 2 Training ---\nAvg Loss     : 0.9728\nTraining CER : 0.8729\nTraining WER : 0.9766\n--- Epoch 2 Validation ---\nValidation Loss : 3.4821\nValidation CER  : 0.8455\nValidation WER  : 1.0519\n\n--- Epoch 3 Training ---\nAvg Loss     : 0.9470\nTraining CER : 0.8502\nTraining WER : 0.9730\n--- Epoch 3 Validation ---\nValidation Loss : 3.0956\nValidation CER  : 0.9739\nValidation WER  : 1.4716\n\n--- Epoch 4 Training ---\nAvg Loss     : 0.9255\nTraining CER : 0.8183\nTraining WER : 0.9677\n--- Epoch 4 Validation ---\nValidation Loss : 3.2038\nValidation CER  : 0.8163\nValidation WER  : 1.1827\n\n--- Epoch 5 Training ---\nAvg Loss     : 0.9222\nTraining CER : 0.8247\nTraining WER : 0.9692\n--- Epoch 5 Validation ---\nValidation Loss : 1.4377\nValidation CER  : 0.9148\nValidation WER  : 1.2832\n\n--- Epoch 6 Training ---\nAvg Loss     : 0.9059\nTraining CER : 0.8032\nTraining WER : 0.9653\n--- Epoch 6 Validation ---\nValidation Loss : 1.3523\nValidation CER  : 0.8075\nValidation WER  : 1.0182\n\n--- Epoch 7 Training ---\nAvg Loss     : 0.8925\nTraining CER : 0.7901\nTraining WER : 0.9635\n--- Epoch 7 Validation ---\nValidation Loss : 1.1346\nValidation CER  : 0.8175\nValidation WER  : 1.0293\n\n--- Epoch 8 Training ---\nAvg Loss     : 0.8838\nTraining CER : 0.7831\nTraining WER : 0.9615\n--- Epoch 8 Validation ---\nValidation Loss : 1.1027\nValidation CER  : 0.8466\nValidation WER  : 1.0559\n\n--- Epoch 9 Training ---\nAvg Loss     : 0.8761\nTraining CER : 0.7784\nTraining WER : 0.9592\n--- Epoch 9 Validation ---\nValidation Loss : 0.9989\nValidation CER  : 0.7865\nValidation WER  : 1.0037\n\n--- Epoch 10 Training ---\nAvg Loss     : 0.8655\nTraining CER : 0.7714\nTraining WER : 0.9554\n--- Epoch 10 Validation ---\nValidation Loss : 0.9949\nValidation CER  : 0.7738\nValidation WER  : 1.0020\n\n--- Epoch 11 Training ---\nAvg Loss     : 0.8556\nTraining CER : 0.7642\nTraining WER : 0.9522\n--- Epoch 11 Validation ---\nValidation Loss : 0.9722\nValidation CER  : 0.7737\nValidation WER  : 0.9783\n\n--- Epoch 12 Training ---\nAvg Loss     : 0.8446\nTraining CER : 0.7591\nTraining WER : 0.9486\n--- Epoch 12 Validation ---\nValidation Loss : 0.9754\nValidation CER  : 0.7668\nValidation WER  : 0.9564\n\n--- Epoch 13 Training ---\nAvg Loss     : 0.8342\nTraining CER : 0.7511\nTraining WER : 0.9445\n--- Epoch 13 Validation ---\nValidation Loss : 0.9712\nValidation CER  : 0.7758\nValidation WER  : 1.0097\n\n--- Epoch 14 Training ---\nAvg Loss     : 0.8202\nTraining CER : 0.7442\nTraining WER : 0.9371\n--- Epoch 14 Validation ---\nValidation Loss : 0.9777\nValidation CER  : 0.7889\nValidation WER  : 0.9863\n\n--- Epoch 15 Training ---\nAvg Loss     : 0.8091\nTraining CER : 0.7415\nTraining WER : 0.9356\n--- Epoch 15 Validation ---\nValidation Loss : 0.9753\nValidation CER  : 0.7785\nValidation WER  : 1.0085\n\n--- Epoch 16 Training ---\nAvg Loss     : 0.7959\nTraining CER : 0.7337\nTraining WER : 0.9294\n--- Epoch 16 Validation ---\nValidation Loss : 0.9683\nValidation CER  : 0.7799\nValidation WER  : 0.9721\n\n--- Epoch 17 Training ---\nAvg Loss     : 0.7819\nTraining CER : 0.7273\nTraining WER : 0.9246\n--- Epoch 17 Validation ---\nValidation Loss : 0.9803\nValidation CER  : 0.7848\nValidation WER  : 0.9942\n\n--- Epoch 18 Training ---\nAvg Loss     : 0.7681\nTraining CER : 0.7196\nTraining WER : 0.9168\n--- Epoch 18 Validation ---\nValidation Loss : 0.9785\nValidation CER  : 0.7813\nValidation WER  : 0.9815\n\n--- Epoch 19 Training ---\nAvg Loss     : 0.7517\nTraining CER : 0.7123\nTraining WER : 0.9098\n--- Epoch 19 Validation ---\nValidation Loss : 0.9862\nValidation CER  : 0.7732\nValidation WER  : 0.9906\n\n--- Epoch 20 Training ---\nAvg Loss     : 0.7363\nTraining CER : 0.7045\nTraining WER : 0.9028\n--- Epoch 20 Validation ---\nValidation Loss : 1.0003\nValidation CER  : 0.7848\nValidation WER  : 1.0114\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Sauvegarder uniquement les poids du modèle\ntorch.save(model.state_dict(), \"handwriting_gpt2_resnet.pt\")\n#from transformers import GPT2Tokenizer\n#tokenizer = GPT2Tokenizer.from_pretrained(\"aragpt2-base\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:14:14.556455Z","iopub.execute_input":"2025-05-12T14:14:14.556688Z","iopub.status.idle":"2025-05-12T14:14:15.421607Z","shell.execute_reply.started":"2025-05-12T14:14:14.556669Z","shell.execute_reply":"2025-05-12T14:14:15.420819Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Prediction Function\ndef predict(image_path, model, tokenizer):\n    transform = transforms.Compose([\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n    ])\n    image = Image.open(image_path).convert('RGB')\n    image = transform(image).unsqueeze(0).to(device)\n    \n    with torch.no_grad():\n        features = model.cnn(image).unsqueeze(1)\n        generated = model.gpt2.generate(\n            inputs_embeds=features,\n            max_length=128,\n            num_beams=5,\n            early_stopping=True\n        )\n    return tokenizer.decode(generated[0], skip_special_tokens=True)\n\n# Test Prediction\ntest_img = \"/kaggle/input/khatt-arabic-hand-written-lines/images/AHTD3A0001_Para1_4.jpg\"\nprint(\"Predicted Text:\", predict(test_img, model, tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:16:47.707243Z","iopub.execute_input":"2025-05-12T14:16:47.707969Z","iopub.status.idle":"2025-05-12T14:16:48.029328Z","shell.execute_reply.started":"2025-05-12T14:16:47.707944Z","shell.execute_reply":"2025-05-12T14:16:48.028587Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Predicted Text:  أفهمض مثل بغ�ض الضابط الضابط لز له الضابطمتك سألت سألت راجح راجح راجح بلغ بلغ بلغ بلغ بلغ بلغ\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}