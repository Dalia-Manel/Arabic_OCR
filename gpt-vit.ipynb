{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8232682,"sourceType":"datasetVersion","datasetId":4882457},{"sourceId":11683054,"sourceType":"datasetVersion","datasetId":7332596},{"sourceId":11764017,"sourceType":"datasetVersion","datasetId":7385338}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Arabic Handwritten Text Recognition\n## Optimized for WER < 0.6 and CER < 0.7","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T10:17:19.024716Z","iopub.execute_input":"2025-05-12T10:17:19.025003Z","iopub.status.idle":"2025-05-12T10:17:22.990417Z","shell.execute_reply.started":"2025-05-12T10:17:19.024980Z","shell.execute_reply":"2025-05-12T10:17:22.989709Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torchvision datasets --quiet\nimport torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom torch.optim import AdamW  # PyTorch's built-in AdamW\nfrom torchvision.models import resnet18\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport os\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T10:17:22.991635Z","iopub.execute_input":"2025-05-12T10:17:22.991881Z","iopub.status.idle":"2025-05-12T10:19:00.313546Z","shell.execute_reply.started":"2025-05-12T10:17:22.991838Z","shell.execute_reply":"2025-05-12T10:19:00.312974Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-05-12 10:18:48.503970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747045128.708408      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747045128.765786      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom torchvision import transforms\n\nclass HandwritingDataset(Dataset):\n    def __init__(self, images_dir, labels_dir, tokenizer, transform=None):\n        self.images_dir = images_dir\n        self.labels_dir = labels_dir\n        self.tokenizer = tokenizer\n        \n        # Transform par défaut pour ViT si non fourni\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.5, 0.5, 0.5],\n                std=[0.5, 0.5, 0.5]\n            )\n        ])\n\n        # Lister les fichiers\n        self.image_files = sorted([\n            f for f in os.listdir(images_dir)\n            if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n        ])\n        self.label_files = sorted([\n            f for f in os.listdir(labels_dir)\n            if f.lower().endswith('.txt')\n        ])\n\n        # Validation stricte des noms (sans extension)\n        image_stems = [os.path.splitext(f)[0] for f in self.image_files]\n        label_stems = [os.path.splitext(f)[0] for f in self.label_files]\n\n        assert image_stems == label_stems, \\\n            f\"Les noms d'images et de labels ne correspondent pas:\\n{image_stems[:5]} vs {label_stems[:5]}\"\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        # Charger l'image\n        img_path = os.path.join(self.images_dir, self.image_files[idx])\n        image = Image.open(img_path).convert('RGB')\n        image = self.transform(image)\n\n        # Charger le label\n        lbl_path = os.path.join(self.labels_dir, self.label_files[idx])\n        with open(lbl_path, 'r', encoding='windows-1256') as f:\n            text = f.read().strip()\n\n        # Tokenization\n        inputs = self.tokenizer(\n            text,\n            return_tensors='pt',\n            padding='max_length',\n            max_length=128,\n            truncation=True\n        )\n\n        return {\n            'pixel_values': image,\n            'input_ids': inputs['input_ids'].squeeze(0),\n            'attention_mask': inputs['attention_mask'].squeeze(0),\n            'labels': inputs['input_ids'].squeeze(0),\n            'raw_text': text\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T10:21:47.945563Z","iopub.execute_input":"2025-05-12T10:21:47.946564Z","iopub.status.idle":"2025-05-12T10:21:47.955283Z","shell.execute_reply.started":"2025-05-12T10:21:47.946537Z","shell.execute_reply":"2025-05-12T10:21:47.954565Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from torchvision import transforms\nfrom transformers import GPT2Tokenizer\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.5, 0.5, 0.5],  # ViT expects normalized input\n        std=[0.5, 0.5, 0.5]\n    )\n])\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"aubmindlab/aragpt2-base\")\ntokenizer.pad_token = tokenizer.eos_token\n\ndataset = HandwritingDataset(\n    images_dir=\"/kaggle/input/khatt-arabic-hand-written-lines/images\",\n    labels_dir=\"/kaggle/input/khatt-arabic-hand-written-lines/labels\",\n    tokenizer=tokenizer,\n    transform=transform\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:54:40.003297Z","iopub.execute_input":"2025-05-10T17:54:40.003598Z","iopub.status.idle":"2025-05-10T17:54:42.839387Z","shell.execute_reply.started":"2025-05-10T17:54:40.003574Z","shell.execute_reply":"2025-05-10T17:54:42.838836Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4ad8328ef914aa2841fed40cc443e89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec6b3afda0424801bba14de998ac79d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/4.52M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d13d41556a04e4fb4e769d98fbf1a24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5ab7bfb93514f838e55a761334ac2eb"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import ViTModel, GPT2LMHeadModel\n\nclass HandwritingGPT2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Charger ViT\n        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n        \n        # Geler le ViT\n        for param in self.vit.parameters():\n            param.requires_grad = False\n        \n        # Adapter la sortie de ViT à GPT2 (ViT hidden_size = 768 → GPT2 = 768, donc optionnel ici)\n        self.linear = nn.Linear(self.vit.config.hidden_size, 768)\n        \n        # Charger GPT2\n        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"aubmindlab/aragpt2-base\")\n        \n    def forward(self, pixel_values, input_ids=None, attention_mask=None, labels=None):\n        # Extraire les features de l'image avec ViT\n        outputs = self.vit(pixel_values=pixel_values)\n        features = outputs.pooler_output  # (batch_size, hidden_size)\n\n        # Adapter les dimensions à GPT2\n        features = self.linear(features)  # (batch_size, 768)\n\n        # Étendre à (batch_size, seq_len, 768)\n        if input_ids is not None:\n            seq_len = input_ids.shape[1]\n            features = features.unsqueeze(1).expand(-1, seq_len, -1)\n\n        return self.gpt2(\n            inputs_embeds=features,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:54:48.269724Z","iopub.execute_input":"2025-05-10T17:54:48.270053Z","iopub.status.idle":"2025-05-10T17:54:48.294327Z","shell.execute_reply.started":"2025-05-10T17:54:48.270028Z","shell.execute_reply":"2025-05-10T17:54:48.293661Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.optim import AdamW\n\n# Tokenizer\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Model\nmodel = HandwritingGPT2().to(device)\n\n# Optimizer: ne pas inclure les paramètres gelés\noptimizer = AdamW(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=5e-5\n)\n\n# Dataloader\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:55:28.282318Z","iopub.execute_input":"2025-05-10T17:55:28.282563Z","iopub.status.idle":"2025-05-10T17:55:33.400208Z","shell.execute_reply.started":"2025-05-10T17:55:28.282546Z","shell.execute_reply":"2025-05-10T17:55:33.399425Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f95d9fd557741ecb2b403c07f2afdd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7505d821134344b2949cd6c1bcc0679d"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d71cc3ded6e40f394d2338700032c30"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Subset\n\n# Séparation train/val\nindices = list(range(len(dataset)))\ntrain_idx, val_idx = train_test_split(indices, test_size=0.1, random_state=42)\n\ntrain_dataset = Subset(dataset, train_idx)\nval_dataset = Subset(dataset, val_idx)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:55:38.019070Z","iopub.execute_input":"2025-05-10T17:55:38.019911Z","iopub.status.idle":"2025-05-10T17:55:38.042026Z","shell.execute_reply.started":"2025-05-10T17:55:38.019878Z","shell.execute_reply":"2025-05-10T17:55:38.041142Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from torchmetrics.text import CharErrorRate, WordErrorRate\nfrom tqdm import tqdm\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import train_test_split\n\n# --- Device and model setup ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = HandwritingGPT2().to(device)\n\n# --- Optimizer (ignoring frozen ViT) ---\noptimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n\n# --- Tokenizer config ---\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# --- Metrics ---\ncer_metric = CharErrorRate().to(device)\nwer_metric = WordErrorRate().to(device)\n\n# --- Train/Val split ---\nindices = list(range(len(dataset)))\ntrain_idx, val_idx = train_test_split(indices, test_size=0.1, random_state=42)\ntrain_dataset = Subset(dataset, train_idx)\nval_dataset = Subset(dataset, val_idx)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8)\n\n# --- Training Loop ---\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    print(f\"\\n=== Epoch {epoch+1}/{num_epochs} ===\")\n\n    # ---------- TRAIN ----------\n    model.train()\n    train_loss = 0\n    train_cers, train_wers = [], []\n\n    for batch in tqdm(train_loader, desc=\"Training\"):\n        pixel_values = batch[\"pixel_values\"].to(device)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        raw_text = batch[\"raw_text\"]\n\n        optimizer.zero_grad()\n        outputs = model(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=input_ids\n        )\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n        # Prédictions\n        with torch.no_grad():\n            vit_features = model.vit(pixel_values=pixel_values).pooler_output\n            features = model.linear(vit_features).unsqueeze(1).expand(-1, input_ids.shape[1], -1)\n            generated = model.gpt2.generate(\n                inputs_embeds=features,\n                max_new_tokens=128,  # corrigé ici\n                num_beams=5,\n                early_stopping=True,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id\n            )\n            preds = [tokenizer.decode(g, skip_special_tokens=True) for g in generated]\n\n            # Metrics\n            batch_cer = cer_metric(preds, raw_text)\n            batch_wer = wer_metric(preds, raw_text)\n            train_cers.append(batch_cer)\n            train_wers.append(batch_wer)\n\n    avg_train_loss = train_loss / len(train_loader)\n    avg_train_cer = torch.stack(train_cers).mean().item()\n    avg_train_wer = torch.stack(train_wers).mean().item()\n    print(f\"\\n[Train] Loss: {avg_train_loss:.4f} | CER: {avg_train_cer:.4f} | WER: {avg_train_wer:.4f}\")\n\n    # ---------- VALIDATION ----------\n    model.eval()\n    val_loss = 0\n    val_cers, val_wers = [], []\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validation\"):\n            pixel_values = batch[\"pixel_values\"].to(device)\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            raw_text = batch[\"raw_text\"]\n\n            outputs = model(\n                pixel_values=pixel_values,\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=input_ids\n            )\n            val_loss += outputs.loss.item()\n\n            vit_features = model.vit(pixel_values=pixel_values).pooler_output\n            features = model.linear(vit_features).unsqueeze(1).expand(-1, input_ids.shape[1], -1)\n            generated = model.gpt2.generate(\n                inputs_embeds=features,\n                max_new_tokens=128,  # corrigé ici \n                num_beams=5,\n                early_stopping=True,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id\n            )\n            preds = [tokenizer.decode(g, skip_special_tokens=True) for g in generated]\n\n            val_cers.append(cer_metric(preds, raw_text))\n            val_wers.append(wer_metric(preds, raw_text))\n\n    avg_val_loss = val_loss / len(val_loader)\n    avg_val_cer = torch.stack(val_cers).mean().item()\n    avg_val_wer = torch.stack(val_wers).mean().item()\n    print(f\"[Validation] Loss: {avg_val_loss:.4f} | CER: {avg_val_cer:.4f} | WER: {avg_val_wer:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:57:17.009213Z","iopub.execute_input":"2025-05-10T17:57:17.009503Z","iopub.status.idle":"2025-05-10T21:23:34.164539Z","shell.execute_reply.started":"2025-05-10T17:57:17.009482Z","shell.execute_reply":"2025-05-10T21:23:34.163784Z"}},"outputs":[{"name":"stdout","text":"\n=== Epoch 1/15 ===\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1280/1280 [14:50<00:00,  1.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[Train] Loss: 1.1532 | CER: 0.9952 | WER: 0.9987\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 143/143 [01:20<00:00,  1.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Validation] Loss: 1.6906 | CER: 0.9726 | WER: 0.9827\n\n=== Epoch 2/15 ===\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1280/1280 [12:29<00:00,  1.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[Train] Loss: 0.9476 | CER: 0.9862 | WER: 0.9951\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 143/143 [01:04<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Validation] Loss: 1.1186 | CER: 0.9648 | WER: 0.9926\n\n=== Epoch 3/15 ===\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1280/1280 [12:28<00:00,  1.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[Train] Loss: 0.9145 | CER: 0.9870 | WER: 0.9941\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 143/143 [01:05<00:00,  2.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Validation] Loss: 1.0316 | CER: 0.9775 | WER: 0.9896\n\n=== Epoch 4/15 ===\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1280/1280 [12:36<00:00,  1.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[Train] Loss: 0.8830 | CER: 0.9876 | WER: 0.9918\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 143/143 [01:06<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Validation] Loss: 0.9915 | CER: 0.9613 | WER: 0.9881\n\n=== Epoch 5/15 ===\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1280/1280 [12:35<00:00,  1.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[Train] Loss: 0.8599 | CER: 0.9896 | WER: 0.9926\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 143/143 [01:05<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Validation] Loss: 0.9480 | CER: 0.9702 | WER: 0.9814\n\n=== Epoch 6/15 ===\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1280/1280 [12:36<00:00,  1.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[Train] Loss: 0.8446 | CER: 0.9939 | WER: 0.9950\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 143/143 [01:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Validation] Loss: 0.9066 | CER: 0.9905 | WER: 0.9948\n\n=== Epoch 7/15 ===\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1280/1280 [12:31<00:00,  1.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[Train] Loss: 0.8288 | CER: 0.9955 | WER: 0.9965\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 143/143 [01:06<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Validation] Loss: 0.9230 | CER: 0.9899 | WER: 0.9951\n\n=== Epoch 8/15 ===\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1280/1280 [12:31<00:00,  1.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[Train] Loss: 0.8115 | CER: 0.9974 | WER: 0.9980\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 143/143 [01:06<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Validation] Loss: 0.8980 | CER: 0.9959 | WER: 0.9946\n\n=== Epoch 9/15 ===\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1280/1280 [12:30<00:00,  1.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[Train] Loss: 0.7972 | CER: 0.9984 | WER: 0.9986\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 143/143 [01:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Validation] Loss: 0.8899 | CER: 0.9985 | WER: 0.9980\n\n=== Epoch 10/15 ===\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1280/1280 [12:27<00:00,  1.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[Train] Loss: 0.7809 | CER: 0.9991 | WER: 0.9992\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 143/143 [01:05<00:00,  2.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Validation] Loss: 0.8852 | CER: 0.9952 | WER: 0.9957\n\n=== Epoch 11/15 ===\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1280/1280 [12:27<00:00,  1.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[Train] Loss: 0.7651 | CER: 0.9990 | WER: 0.9992\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 143/143 [01:05<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Validation] Loss: 0.8886 | CER: 0.9969 | WER: 0.9982\n\n=== Epoch 12/15 ===\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1280/1280 [12:28<00:00,  1.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[Train] Loss: 0.7476 | CER: 0.9987 | WER: 0.9992\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 143/143 [01:04<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Validation] Loss: 0.8913 | CER: 0.9991 | WER: 0.9991\n\n=== Epoch 13/15 ===\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1280/1280 [12:27<00:00,  1.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[Train] Loss: 0.7277 | CER: 0.9993 | WER: 0.9995\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 143/143 [01:04<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Validation] Loss: 0.8790 | CER: 0.9997 | WER: 0.9997\n\n=== Epoch 14/15 ===\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1280/1280 [12:25<00:00,  1.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[Train] Loss: 0.7065 | CER: 0.9991 | WER: 0.9995\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 143/143 [01:04<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Validation] Loss: 0.8957 | CER: 0.9973 | WER: 0.9976\n\n=== Epoch 15/15 ===\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1280/1280 [12:16<00:00,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[Train] Loss: 0.6820 | CER: 0.9994 | WER: 0.9998\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 143/143 [01:03<00:00,  2.24it/s]","output_type":"stream"},{"name":"stdout","text":"[Validation] Loss: 0.9114 | CER: 0.9966 | WER: 0.9977\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Sauvegarder uniquement les poids du modèle\ntorch.save(model.state_dict(), \"handwriting_gpt2_vit.pt\")\n#from transformers import GPT2Tokenizer\n#tokenizer = GPT2Tokenizer.from_pretrained(\"aragpt2-base\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T21:27:30.887026Z","iopub.execute_input":"2025-05-10T21:27:30.887607Z","iopub.status.idle":"2025-05-10T21:27:32.157048Z","shell.execute_reply.started":"2025-05-10T21:27:30.887587Z","shell.execute_reply":"2025-05-10T21:27:32.156368Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from torchvision import transforms\nfrom PIL import Image\n\ndef predict(image_path, model, tokenizer):\n    model.eval()\n\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),  # ✅ adapté pour ViT\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                             std=[0.5, 0.5, 0.5])\n    ])\n\n    image = Image.open(image_path).convert('RGB')\n    image = transform(image).unsqueeze(0).to(device)  # (1, 3, 224, 224)\n\n    with torch.no_grad():\n        vit_features = model.vit(pixel_values=image).pooler_output  # (1, 768)\n        features = model.linear(vit_features).unsqueeze(1)  # (1, 1, 768)\n\n        generated = model.gpt2.generate(\n            inputs_embeds=features,\n            max_new_tokens=128,  # ✅ éviter ValueError\n            num_beams=5,\n            early_stopping=True,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id\n        )\n\n    return tokenizer.decode(generated[0], skip_special_tokens=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T21:28:38.183905Z","iopub.execute_input":"2025-05-10T21:28:38.184180Z","iopub.status.idle":"2025-05-10T21:28:38.190517Z","shell.execute_reply.started":"2025-05-10T21:28:38.184161Z","shell.execute_reply":"2025-05-10T21:28:38.189785Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"test_img = \"/kaggle/input/dataset1/test2.png\"\nprint(\"Predicted:\", predict(test_img, model, tokenizer))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}